{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlhf4TK4d58S"
      },
      "outputs": [],
      "source": [
        "# import and load data\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "import pickle\n",
        "import email\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "\n",
        "import re\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "!pip install simplet5\n",
        "from simplet5 import SimpleT5\n",
        "\n",
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    MT5ForConditionalGeneration,\n",
        "    ByT5Tokenizer,\n",
        "    PreTrainedTokenizer,\n",
        "    T5TokenizerFast as T5Tokenizer,\n",
        "    MT5TokenizerFast as MT5Tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcnBGYLLgKvj"
      },
      "source": [
        "### Data Preparation to feed in to GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load gpt data\n",
        "!gdown --id 1IvheqVMHPsYoA7o__azk0iedOYmt0ABi\n",
        "with open('gpt_data.pickle','rb') as file:\n",
        "    train_data_gpt,test_data_gpt = pickle.load(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJy3VAIcxSE3",
        "outputId": "8b987951-7fbb-4d23-d1ab-8de60c09b479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IvheqVMHPsYoA7o__azk0iedOYmt0ABi\n",
            "To: /content/gpt_data.pickle\n",
            "100% 8.70M/8.70M [00:00<00:00, 83.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "1hpc4mof_Yd-",
        "outputId": "cbc34578-13c7-4a7e-8cb3-01934dd1dddb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Body\n",
              "0                    I take back my dog comment john\n",
              "1  Please take a look at it You may find it usefu...\n",
              "2                                             fyi sg\n",
              "3  Taylor of ENA sent his erequest in last Thursd...\n",
              "4  FYI anyone that deals with CAP please read the...\n",
              "5  I not aware of the volume rate info you used i...\n",
              "6                                 will be back today\n",
              "7  can you please work up an offer for for Do not...\n",
              "8                      come on you are kidding right\n",
              "9  I received the revised opinion from this morni..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1d1852d-2278-42d7-b9da-690350c0120b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I take back my dog comment john</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Please take a look at it You may find it usefu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>fyi sg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Taylor of ENA sent his erequest in last Thursd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>FYI anyone that deals with CAP please read the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I not aware of the volume rate info you used i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>will be back today</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>can you please work up an offer for for Do not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>come on you are kidding right</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I received the revised opinion from this morni...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1d1852d-2278-42d7-b9da-690350c0120b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1d1852d-2278-42d7-b9da-690350c0120b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1d1852d-2278-42d7-b9da-690350c0120b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_data_gpt.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulXvmcX2VbUx"
      },
      "outputs": [],
      "source": [
        "train_data_gpt.to_csv('train_data_gpt.csv',index=False) # we need in file.csv format for gpt2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation to feed in to T5    \n",
        "Load the dataset with Enc_seq and Dec_seq columns which will be also be useful in **Evaluation of GPT-mode**l "
      ],
      "metadata": {
        "id": "XO2zYl8rf2MN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZTrfbUFvd9Hm"
      },
      "outputs": [],
      "source": [
        "# !gdown --id 1cvJp9HTZ5z6FvMl5Q7bCenWVbtqgzYCa\n",
        "with open('Sequence_data.pickle', 'rb') as file:\n",
        "    train_sequences,test_sequences = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gs4cOI0Z2Rje",
        "outputId": "dcef6be1-9618-4975-fd50-24a8707e5fc4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        enc_seq                          dec_seq\n",
              "0            I take back my dog                     comment john\n",
              "1    I take back my dog comment                             john\n",
              "2         Please take a look at  it You may find it useful Vince\n",
              "3      Please take a look at it     You may find it useful Vince\n",
              "4  Please take a look at it You         may find it useful Vince"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c05ea7e-4ae6-44cd-b62b-698d83180a47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>enc_seq</th>\n",
              "      <th>dec_seq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I take back my dog</td>\n",
              "      <td>comment john</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I take back my dog comment</td>\n",
              "      <td>john</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Please take a look at</td>\n",
              "      <td>it You may find it useful Vince</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Please take a look at it</td>\n",
              "      <td>You may find it useful Vince</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please take a look at it You</td>\n",
              "      <td>may find it useful Vince</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c05ea7e-4ae6-44cd-b62b-698d83180a47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c05ea7e-4ae6-44cd-b62b-698d83180a47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c05ea7e-4ae6-44cd-b62b-698d83180a47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A__ypVCM_V_Q"
      },
      "source": [
        "### GPT-2 Modelling (Base-124 Million params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9mBHCj5TiQa",
        "outputId": "55f50656-1639-4c61-e84d-f5bf4b5cda5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 402Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 2.96Mit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 533Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:18, 26.9Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 528Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 3.59Mit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 3.72Mit/s]\n"
          ]
        }
      ],
      "source": [
        "gpt2.mount_gdrive()\n",
        "gpt2.download_gpt2(model_name=\"124M\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3HUB94NqDuL"
      },
      "outputs": [],
      "source": [
        "# IF data is big, we may wanna encode the data first for GPT model\n",
        "\n",
        "# gpt2.encode_csv('train_data_gpt.csv',out_path='csv_encoded.txt')  # add EOS AND BOS Tokens\n",
        "# gpt2.encode_dataset('train_data_gpt.txt')  # encode data to numpy npz\n",
        "\n",
        "# LOAD THE Partially-trained model\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17304e92-1005-422a-e34d-07c173ecb6e7",
        "id": "5-9wG2y2hFWa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 2675731 tokens\n",
            "Training...\n",
            "Saving checkpoint/run1/model-0\n",
            "[5 | 27.13] loss=2.50 avg=2.50\n",
            "[10 | 48.17] loss=2.23 avg=2.37\n",
            "[15 | 69.64] loss=2.20 avg=2.31\n",
            "[20 | 91.56] loss=2.16 avg=2.27\n",
            "[25 | 113.99] loss=2.11 avg=2.24\n",
            "[30 | 136.86] loss=1.97 avg=2.19\n",
            "[35 | 159.51] loss=2.01 avg=2.17\n",
            "[40 | 182.02] loss=1.88 avg=2.13\n",
            "[45 | 204.54] loss=2.00 avg=2.11\n",
            "[50 | 227.20] loss=1.94 avg=2.10\n",
            "[55 | 249.87] loss=1.86 avg=2.07\n",
            "[60 | 272.49] loss=2.06 avg=2.07\n",
            "[65 | 295.08] loss=1.98 avg=2.07\n",
            "[70 | 317.65] loss=1.89 avg=2.05\n",
            "[75 | 340.28] loss=1.86 avg=2.04\n",
            "[80 | 362.94] loss=1.79 avg=2.02\n",
            "[85 | 385.64] loss=1.77 avg=2.01\n",
            "[90 | 408.30] loss=1.92 avg=2.00\n",
            "[95 | 430.94] loss=1.82 avg=1.99\n",
            "[100 | 453.56] loss=1.89 avg=1.99\n",
            "[105 | 476.16] loss=1.82 avg=1.98\n",
            "[110 | 498.80] loss=1.80 avg=1.97\n",
            "[115 | 521.45] loss=1.72 avg=1.96\n",
            "[120 | 544.14] loss=1.80 avg=1.95\n",
            "[125 | 566.82] loss=1.78 avg=1.94\n",
            "[130 | 589.49] loss=1.80 avg=1.93\n",
            "[135 | 612.17] loss=1.78 avg=1.93\n",
            "[140 | 634.81] loss=1.72 avg=1.92\n",
            "[145 | 657.46] loss=1.77 avg=1.91\n",
            "[150 | 680.08] loss=1.73 avg=1.91\n",
            "[155 | 702.72] loss=1.66 avg=1.90\n",
            "[160 | 725.33] loss=1.77 avg=1.89\n",
            "[165 | 747.95] loss=1.64 avg=1.88\n",
            "[170 | 770.58] loss=1.71 avg=1.88\n",
            "[175 | 793.24] loss=1.71 avg=1.87\n",
            "[180 | 815.90] loss=1.68 avg=1.87\n",
            "[185 | 838.58] loss=1.79 avg=1.86\n",
            "[190 | 861.26] loss=1.62 avg=1.86\n",
            "[195 | 883.94] loss=1.71 avg=1.85\n",
            "[200 | 906.64] loss=1.69 avg=1.85\n",
            "======== SAMPLE 1 ========\n",
            ">><|endoftext|>The Office of the Associate Administrator R C Smith W White Office eax<|endoftext|>\n",
            "<|startoftext|\n",
            "\n",
            "[205 | 931.00] loss=1.73 avg=1.84\n",
            "[210 | 953.65] loss=1.67 avg=1.84\n",
            "[215 | 976.32] loss=1.63 avg=1.83\n",
            "[220 | 998.99] loss=1.70 avg=1.83\n",
            "[225 | 1021.67] loss=1.79 avg=1.83\n",
            "[230 | 1044.36] loss=1.70 avg=1.82\n",
            "[235 | 1067.04] loss=1.55 avg=1.82\n",
            "[240 | 1089.70] loss=1.66 avg=1.81\n",
            "[245 | 1112.33] loss=1.63 avg=1.81\n",
            "[250 | 1134.97] loss=1.59 avg=1.80\n",
            "[255 | 1157.59] loss=1.70 avg=1.80\n",
            "[260 | 1180.24] loss=1.66 avg=1.80\n",
            "[265 | 1202.88] loss=1.57 avg=1.79\n",
            "[270 | 1225.56] loss=1.61 avg=1.79\n",
            "[275 | 1248.23] loss=1.64 avg=1.78\n",
            "[280 | 1270.87] loss=1.63 avg=1.78\n",
            "[285 | 1293.53] loss=1.61 avg=1.78\n",
            "[290 | 1316.19] loss=1.56 avg=1.77\n",
            "[295 | 1338.85] loss=1.58 avg=1.77\n",
            "[300 | 1361.49] loss=1.67 avg=1.76\n",
            "[305 | 1384.15] loss=1.68 avg=1.76\n",
            "[310 | 1406.82] loss=1.62 avg=1.76\n",
            "[315 | 1429.48] loss=1.60 avg=1.76\n",
            "[320 | 1452.14] loss=1.61 avg=1.75\n",
            "[325 | 1474.81] loss=1.63 avg=1.75\n",
            "[330 | 1497.50] loss=1.58 avg=1.75\n",
            "[335 | 1520.16] loss=1.58 avg=1.74\n",
            "[340 | 1542.83] loss=1.64 avg=1.74\n",
            "[345 | 1565.49] loss=1.61 avg=1.74\n",
            "[350 | 1588.14] loss=1.56 avg=1.74\n",
            "[355 | 1610.82] loss=1.54 avg=1.73\n",
            "[360 | 1633.48] loss=1.63 avg=1.73\n",
            "[365 | 1656.14] loss=1.46 avg=1.72\n",
            "[370 | 1678.82] loss=1.57 avg=1.72\n",
            "[375 | 1701.48] loss=1.53 avg=1.72\n",
            "[380 | 1724.14] loss=1.52 avg=1.71\n",
            "[385 | 1746.80] loss=1.50 avg=1.71\n",
            "[390 | 1769.45] loss=1.60 avg=1.71\n",
            "[395 | 1792.08] loss=1.50 avg=1.70\n",
            "[400 | 1814.74] loss=1.58 avg=1.70\n",
            "======== SAMPLE 1 ========\n",
            " you have any new questions<|endoftext|>\n",
            "<|startoftext|>I hope you enjoyed this<|endoftext|\n",
            "\n",
            "[405 | 1837.73] loss=1.55 avg=1.70\n",
            "[410 | 1860.36] loss=1.57 avg=1.70\n",
            "[415 | 1883.01] loss=1.58 avg=1.69\n",
            "[420 | 1905.67] loss=1.46 avg=1.69\n",
            "[425 | 1928.31] loss=1.59 avg=1.69\n",
            "[430 | 1950.96] loss=1.49 avg=1.69\n",
            "[435 | 1973.62] loss=1.52 avg=1.68\n",
            "[440 | 1996.26] loss=1.54 avg=1.68\n",
            "[445 | 2018.88] loss=1.46 avg=1.68\n",
            "[450 | 2041.51] loss=1.61 avg=1.68\n",
            "[455 | 2064.15] loss=1.50 avg=1.67\n",
            "[460 | 2086.82] loss=1.62 avg=1.67\n",
            "[465 | 2109.48] loss=1.57 avg=1.67\n",
            "[470 | 2132.12] loss=1.51 avg=1.67\n",
            "[475 | 2154.79] loss=1.53 avg=1.67\n",
            "[480 | 2177.46] loss=1.52 avg=1.66\n",
            "[485 | 2200.10] loss=1.49 avg=1.66\n",
            "[490 | 2222.74] loss=1.55 avg=1.66\n",
            "[495 | 2245.39] loss=1.52 avg=1.66\n",
            "[500 | 2268.06] loss=1.48 avg=1.65\n",
            "Saving checkpoint/run1/model-500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1054: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "[505 | 2293.12] loss=1.55 avg=1.65\n",
            "[510 | 2315.81] loss=1.46 avg=1.65\n",
            "[515 | 2338.52] loss=1.43 avg=1.65\n",
            "[520 | 2361.17] loss=1.47 avg=1.64\n",
            "[525 | 2383.80] loss=1.44 avg=1.64\n",
            "[530 | 2406.42] loss=1.38 avg=1.64\n",
            "[535 | 2429.05] loss=1.35 avg=1.63\n",
            "[540 | 2451.70] loss=1.43 avg=1.63\n",
            "[545 | 2474.37] loss=1.38 avg=1.62\n",
            "[550 | 2497.06] loss=1.37 avg=1.62\n",
            "[555 | 2519.74] loss=1.47 avg=1.62\n",
            "[560 | 2542.42] loss=1.43 avg=1.62\n",
            "[565 | 2565.12] loss=1.39 avg=1.61\n",
            "[570 | 2587.79] loss=1.45 avg=1.61\n",
            "[575 | 2610.47] loss=1.41 avg=1.61\n",
            "[580 | 2633.15] loss=1.43 avg=1.60\n",
            "[585 | 2655.81] loss=1.43 avg=1.60\n",
            "[590 | 2678.47] loss=1.38 avg=1.60\n",
            "[595 | 2701.13] loss=1.37 avg=1.60\n",
            "[600 | 2723.79] loss=1.38 avg=1.59\n",
            "======== SAMPLE 1 ========\n",
            "endoftext|>\n",
            "<|startoftext|>fyi<|endoftext|>\n",
            "<|startoftext|>Please\n",
            "\n",
            "[605 | 2746.78] loss=1.52 avg=1.59\n",
            "[610 | 2769.45] loss=1.40 avg=1.59\n",
            "[615 | 2792.11] loss=1.43 avg=1.59\n",
            "[620 | 2814.77] loss=1.39 avg=1.58\n",
            "[625 | 2837.42] loss=1.37 avg=1.58\n",
            "[630 | 2860.08] loss=1.34 avg=1.58\n",
            "[635 | 2882.73] loss=1.43 avg=1.58\n",
            "[640 | 2905.38] loss=1.38 avg=1.57\n",
            "[645 | 2928.04] loss=1.37 avg=1.57\n",
            "[650 | 2950.69] loss=1.39 avg=1.57\n",
            "[655 | 2973.34] loss=1.34 avg=1.56\n",
            "[660 | 2995.98] loss=1.41 avg=1.56\n",
            "[665 | 3018.64] loss=1.40 avg=1.56\n",
            "[670 | 3041.30] loss=1.37 avg=1.56\n",
            "[675 | 3063.95] loss=1.32 avg=1.55\n",
            "[680 | 3086.60] loss=1.42 avg=1.55\n",
            "[685 | 3109.25] loss=1.30 avg=1.55\n",
            "[690 | 3131.91] loss=1.29 avg=1.55\n",
            "[695 | 3154.55] loss=1.25 avg=1.54\n",
            "[700 | 3177.23] loss=1.37 avg=1.54\n",
            "[705 | 3199.89] loss=1.40 avg=1.54\n",
            "[710 | 3222.57] loss=1.39 avg=1.54\n",
            "[715 | 3245.26] loss=1.35 avg=1.53\n",
            "[720 | 3267.93] loss=1.35 avg=1.53\n",
            "[725 | 3290.60] loss=1.31 avg=1.53\n",
            "[730 | 3313.28] loss=1.32 avg=1.53\n",
            "[735 | 3335.95] loss=1.27 avg=1.52\n",
            "[740 | 3358.63] loss=1.31 avg=1.52\n",
            "[745 | 3381.31] loss=1.31 avg=1.52\n",
            "[750 | 3403.96] loss=1.22 avg=1.51\n",
            "[755 | 3426.59] loss=1.33 avg=1.51\n",
            "[760 | 3449.23] loss=1.36 avg=1.51\n",
            "[765 | 3471.87] loss=1.30 avg=1.51\n",
            "[770 | 3494.51] loss=1.38 avg=1.50\n",
            "[775 | 3517.16] loss=1.29 avg=1.50\n",
            "[780 | 3539.87] loss=1.35 avg=1.50\n",
            "[785 | 3562.60] loss=1.35 avg=1.50\n",
            "[790 | 3585.28] loss=1.37 avg=1.50\n",
            "[795 | 3607.93] loss=1.43 avg=1.50\n",
            "[800 | 3630.57] loss=1.29 avg=1.49\n",
            "======== SAMPLE 1 ========\n",
            "<|endoftext|>\n",
            "<|startoftext|>i forgot the first check mark i can do dallas tonight<|endof\n",
            "\n",
            "[805 | 3653.50] loss=1.30 avg=1.49\n",
            "[810 | 3676.14] loss=1.30 avg=1.49\n",
            "[815 | 3698.78] loss=1.27 avg=1.49\n",
            "[820 | 3721.46] loss=1.19 avg=1.48\n",
            "[825 | 3744.14] loss=1.26 avg=1.48\n",
            "[830 | 3766.81] loss=1.30 avg=1.48\n",
            "[835 | 3789.49] loss=1.30 avg=1.47\n",
            "[840 | 3812.18] loss=1.21 avg=1.47\n",
            "[845 | 3834.86] loss=1.34 avg=1.47\n",
            "[850 | 3857.53] loss=1.28 avg=1.47\n",
            "[855 | 3880.20] loss=1.18 avg=1.46\n",
            "[860 | 3902.84] loss=1.26 avg=1.46\n",
            "[865 | 3925.49] loss=1.36 avg=1.46\n",
            "[870 | 3948.15] loss=1.33 avg=1.46\n",
            "[875 | 3970.86] loss=1.29 avg=1.46\n",
            "[880 | 3993.54] loss=1.27 avg=1.45\n",
            "[885 | 4016.16] loss=1.14 avg=1.45\n",
            "[890 | 4038.73] loss=1.30 avg=1.45\n",
            "[895 | 4061.39] loss=1.31 avg=1.45\n",
            "[900 | 4084.15] loss=1.24 avg=1.44\n",
            "[905 | 4106.78] loss=1.30 avg=1.44\n",
            "[910 | 4129.39] loss=1.20 avg=1.44\n",
            "[915 | 4152.05] loss=1.21 avg=1.44\n",
            "[920 | 4174.75] loss=1.26 avg=1.43\n",
            "[925 | 4197.47] loss=1.28 avg=1.43\n",
            "[930 | 4220.13] loss=1.33 avg=1.43\n",
            "[935 | 4242.77] loss=1.20 avg=1.43\n",
            "[940 | 4265.41] loss=1.17 avg=1.43\n",
            "[945 | 4288.04] loss=1.20 avg=1.42\n",
            "[950 | 4310.66] loss=1.19 avg=1.42\n",
            "[955 | 4333.29] loss=1.22 avg=1.42\n",
            "[960 | 4355.93] loss=1.17 avg=1.42\n",
            "[965 | 4378.56] loss=1.16 avg=1.41\n",
            "[970 | 4401.21] loss=1.09 avg=1.41\n",
            "[975 | 4423.89] loss=1.28 avg=1.41\n",
            "[980 | 4446.57] loss=1.23 avg=1.41\n",
            "[985 | 4469.22] loss=1.27 avg=1.40\n",
            "[990 | 4491.89] loss=1.25 avg=1.40\n",
            "[995 | 4514.58] loss=1.23 avg=1.40\n",
            "[1000 | 4537.29] loss=1.35 avg=1.40\n",
            "Saving checkpoint/run1/model-1000\n",
            "======== SAMPLE 1 ========\n",
            " as your calendar would like to discuss this subject with your group and give you the details Best Regards tm<|endoftext|>\n",
            "<\n",
            "\n",
            "[1005 | 4562.91] loss=1.28 avg=1.40\n",
            "[1010 | 4585.61] loss=1.18 avg=1.40\n",
            "[1015 | 4608.34] loss=1.19 avg=1.39\n",
            "[1020 | 4631.03] loss=1.24 avg=1.39\n",
            "[1025 | 4653.65] loss=1.19 avg=1.39\n",
            "[1030 | 4676.30] loss=1.20 avg=1.39\n",
            "[1035 | 4698.94] loss=1.20 avg=1.38\n",
            "[1040 | 4721.61] loss=1.11 avg=1.38\n",
            "[1045 | 4744.26] loss=1.18 avg=1.38\n",
            "[1050 | 4766.95] loss=1.11 avg=1.38\n",
            "[1055 | 4789.60] loss=1.25 avg=1.37\n",
            "[1060 | 4812.28] loss=1.23 avg=1.37\n",
            "[1065 | 4834.96] loss=1.20 avg=1.37\n",
            "[1070 | 4857.64] loss=1.19 avg=1.37\n",
            "[1075 | 4880.33] loss=1.18 avg=1.37\n",
            "[1080 | 4903.02] loss=1.18 avg=1.36\n",
            "[1085 | 4925.71] loss=1.13 avg=1.36\n",
            "[1090 | 4948.42] loss=1.13 avg=1.36\n",
            "[1095 | 4971.10] loss=1.17 avg=1.36\n",
            "[1100 | 4993.78] loss=1.16 avg=1.36\n",
            "[1105 | 5016.46] loss=1.10 avg=1.35\n",
            "[1110 | 5039.16] loss=1.14 avg=1.35\n",
            "[1115 | 5061.84] loss=1.14 avg=1.35\n",
            "[1120 | 5084.51] loss=1.20 avg=1.35\n",
            "[1125 | 5107.18] loss=1.24 avg=1.34\n",
            "[1130 | 5129.83] loss=1.21 avg=1.34\n",
            "[1135 | 5152.48] loss=1.21 avg=1.34\n",
            "[1140 | 5175.12] loss=1.06 avg=1.34\n",
            "[1145 | 5197.75] loss=1.06 avg=1.34\n",
            "[1150 | 5220.38] loss=1.03 avg=1.33\n",
            "[1155 | 5243.02] loss=1.07 avg=1.33\n",
            "[1160 | 5265.70] loss=1.16 avg=1.33\n",
            "[1165 | 5288.37] loss=1.07 avg=1.32\n",
            "[1170 | 5311.05] loss=1.06 avg=1.32\n",
            "[1175 | 5333.72] loss=1.18 avg=1.32\n",
            "[1180 | 5356.41] loss=1.12 avg=1.32\n",
            "[1185 | 5379.10] loss=1.11 avg=1.32\n",
            "[1190 | 5401.79] loss=1.04 avg=1.31\n",
            "[1195 | 5424.47] loss=1.04 avg=1.31\n",
            "[1200 | 5447.16] loss=1.11 avg=1.31\n",
            "======== SAMPLE 1 ========\n",
            "text|>\n",
            "<|startoftext|>How far is that from the desk to you<|endoftext|>\n",
            "<|start\n",
            "\n",
            "[1205 | 5470.15] loss=1.04 avg=1.30\n",
            "[1210 | 5492.80] loss=1.07 avg=1.30\n",
            "[1215 | 5515.43] loss=1.03 avg=1.30\n",
            "[1220 | 5538.06] loss=1.11 avg=1.30\n",
            "[1225 | 5560.70] loss=1.17 avg=1.30\n",
            "[1230 | 5583.37] loss=1.00 avg=1.29\n",
            "[1235 | 5606.06] loss=1.10 avg=1.29\n",
            "[1240 | 5628.73] loss=1.10 avg=1.29\n",
            "[1245 | 5651.41] loss=1.07 avg=1.29\n",
            "[1250 | 5674.11] loss=0.99 avg=1.28\n",
            "[1255 | 5696.78] loss=1.07 avg=1.28\n",
            "[1260 | 5719.44] loss=0.99 avg=1.28\n",
            "[1265 | 5742.10] loss=1.09 avg=1.28\n",
            "[1270 | 5764.72] loss=1.06 avg=1.27\n",
            "[1275 | 5787.35] loss=0.97 avg=1.27\n",
            "[1280 | 5810.01] loss=0.99 avg=1.27\n",
            "[1285 | 5832.65] loss=1.00 avg=1.26\n",
            "[1290 | 5855.30] loss=1.00 avg=1.26\n",
            "[1295 | 5877.97] loss=0.99 avg=1.26\n",
            "[1300 | 5900.65] loss=1.07 avg=1.26\n",
            "[1305 | 5923.34] loss=1.12 avg=1.25\n",
            "[1310 | 5946.03] loss=1.00 avg=1.25\n",
            "[1315 | 5968.71] loss=1.06 avg=1.25\n",
            "[1320 | 5991.38] loss=1.03 avg=1.25\n",
            "[1325 | 6014.05] loss=0.99 avg=1.24\n",
            "[1330 | 6036.71] loss=1.05 avg=1.24\n",
            "[1335 | 6059.34] loss=0.96 avg=1.24\n",
            "[1340 | 6082.01] loss=1.01 avg=1.24\n",
            "[1345 | 6104.68] loss=0.98 avg=1.23\n",
            "[1350 | 6127.36] loss=1.02 avg=1.23\n",
            "[1355 | 6150.04] loss=0.98 avg=1.23\n",
            "[1360 | 6172.70] loss=1.11 avg=1.23\n",
            "[1365 | 6195.36] loss=0.99 avg=1.23\n",
            "[1370 | 6218.05] loss=1.03 avg=1.22\n",
            "[1375 | 6240.72] loss=1.03 avg=1.22\n",
            "[1380 | 6263.40] loss=0.94 avg=1.22\n",
            "[1385 | 6286.08] loss=0.96 avg=1.22\n",
            "[1390 | 6308.76] loss=1.02 avg=1.21\n",
            "[1395 | 6331.42] loss=1.00 avg=1.21\n",
            "[1400 | 6354.08] loss=1.12 avg=1.21\n",
            "======== SAMPLE 1 ========\n",
            " click for the attached<|endoftext|>\n",
            "<|startoftext|>Yes When do I get  is and  is names Thanks\n",
            "\n",
            "[1405 | 6377.06] loss=0.89 avg=1.21\n",
            "[1410 | 6399.70] loss=1.05 avg=1.20\n",
            "[1415 | 6422.35] loss=1.01 avg=1.20\n",
            "[1420 | 6445.01] loss=1.02 avg=1.20\n",
            "[1425 | 6467.68] loss=0.94 avg=1.20\n",
            "[1430 | 6490.36] loss=0.97 avg=1.20\n",
            "[1435 | 6513.03] loss=0.95 avg=1.19\n",
            "[1440 | 6535.72] loss=0.96 avg=1.19\n",
            "[1445 | 6558.40] loss=0.98 avg=1.19\n",
            "[1450 | 6581.06] loss=1.02 avg=1.19\n",
            "[1455 | 6603.72] loss=0.98 avg=1.18\n",
            "[1460 | 6626.37] loss=0.96 avg=1.18\n",
            "[1465 | 6649.03] loss=0.90 avg=1.18\n",
            "[1470 | 6671.70] loss=0.96 avg=1.18\n",
            "[1475 | 6694.37] loss=0.99 avg=1.18\n",
            "[1480 | 6717.04] loss=1.08 avg=1.17\n",
            "[1485 | 6739.71] loss=0.99 avg=1.17\n",
            "[1490 | 6762.39] loss=0.94 avg=1.17\n",
            "[1495 | 6785.09] loss=0.98 avg=1.17\n",
            "[1500 | 6807.74] loss=0.93 avg=1.17\n",
            "Saving checkpoint/run1/model-1500\n",
            "[1505 | 6832.79] loss=0.92 avg=1.16\n",
            "[1510 | 6855.50] loss=0.98 avg=1.16\n",
            "[1515 | 6878.20] loss=0.96 avg=1.16\n",
            "[1520 | 6900.86] loss=0.90 avg=1.16\n",
            "[1525 | 6923.49] loss=0.81 avg=1.15\n",
            "[1530 | 6946.09] loss=0.98 avg=1.15\n",
            "[1535 | 6968.72] loss=0.98 avg=1.15\n",
            "[1540 | 6991.36] loss=0.94 avg=1.15\n",
            "[1545 | 7013.99] loss=0.95 avg=1.14\n",
            "[1550 | 7036.64] loss=0.92 avg=1.14\n",
            "[1555 | 7059.33] loss=1.04 avg=1.14\n",
            "[1560 | 7082.02] loss=0.89 avg=1.14\n",
            "[1565 | 7104.70] loss=0.86 avg=1.14\n",
            "[1570 | 7127.38] loss=0.96 avg=1.13\n",
            "[1575 | 7150.08] loss=0.93 avg=1.13\n",
            "[1580 | 7172.75] loss=0.79 avg=1.13\n",
            "[1585 | 7195.44] loss=0.88 avg=1.13\n",
            "[1590 | 7218.12] loss=0.89 avg=1.12\n",
            "[1595 | 7240.80] loss=0.83 avg=1.12\n",
            "[1600 | 7263.47] loss=0.93 avg=1.12\n",
            "======== SAMPLE 1 ========\n",
            " LOG for for the confusion Regards<|endoftext|>\n",
            "<|startoftext|>CALENDAR APPOINTMENT Terry\n",
            "\n",
            "[1605 | 7286.50] loss=0.89 avg=1.12\n",
            "[1610 | 7309.18] loss=0.83 avg=1.11\n",
            "[1615 | 7331.86] loss=0.84 avg=1.11\n",
            "[1620 | 7354.49] loss=0.78 avg=1.11\n",
            "[1625 | 7377.13] loss=0.87 avg=1.10\n",
            "[1630 | 7399.77] loss=0.87 avg=1.10\n",
            "[1635 | 7422.42] loss=0.87 avg=1.10\n",
            "[1640 | 7445.09] loss=0.82 avg=1.10\n",
            "[1645 | 7467.74] loss=0.84 avg=1.09\n",
            "[1650 | 7490.42] loss=0.86 avg=1.09\n",
            "[1655 | 7513.10] loss=0.83 avg=1.09\n",
            "[1660 | 7535.77] loss=0.82 avg=1.09\n",
            "[1665 | 7558.44] loss=0.82 avg=1.08\n",
            "[1670 | 7581.10] loss=0.98 avg=1.08\n",
            "[1675 | 7603.76] loss=0.91 avg=1.08\n",
            "[1680 | 7626.43] loss=0.89 avg=1.08\n",
            "[1685 | 7649.07] loss=0.77 avg=1.07\n",
            "[1690 | 7671.71] loss=0.80 avg=1.07\n",
            "[1695 | 7694.34] loss=0.77 avg=1.07\n",
            "[1700 | 7716.96] loss=0.92 avg=1.07\n",
            "[1705 | 7739.58] loss=0.85 avg=1.06\n",
            "[1710 | 7762.23] loss=0.86 avg=1.06\n",
            "[1715 | 7784.88] loss=0.81 avg=1.06\n",
            "[1720 | 7807.51] loss=0.85 avg=1.06\n",
            "[1725 | 7830.16] loss=0.80 avg=1.06\n",
            "[1730 | 7852.82] loss=0.93 avg=1.05\n",
            "[1735 | 7875.48] loss=0.87 avg=1.05\n",
            "[1740 | 7898.15] loss=0.83 avg=1.05\n",
            "[1745 | 7920.81] loss=0.83 avg=1.05\n",
            "[1750 | 7943.48] loss=0.84 avg=1.05\n",
            "[1755 | 7966.14] loss=0.83 avg=1.04\n",
            "[1760 | 7988.83] loss=0.81 avg=1.04\n",
            "[1765 | 8011.49] loss=0.76 avg=1.04\n",
            "[1770 | 8034.16] loss=0.75 avg=1.03\n",
            "[1775 | 8056.82] loss=0.88 avg=1.03\n",
            "[1780 | 8079.46] loss=0.85 avg=1.03\n",
            "[1785 | 8102.14] loss=0.77 avg=1.03\n",
            "[1790 | 8124.81] loss=0.84 avg=1.03\n",
            "[1795 | 8147.49] loss=0.80 avg=1.02\n",
            "[1800 | 8170.16] loss=0.84 avg=1.02\n",
            "======== SAMPLE 1 ========\n",
            " resources<|endoftext|>\n",
            "<|startoftext|>Did you get a chance to talk to Sara about this Please let me know\n",
            "\n",
            "[1805 | 8193.10] loss=0.72 avg=1.02\n",
            "[1810 | 8215.71] loss=0.79 avg=1.02\n",
            "[1815 | 8238.34] loss=0.87 avg=1.02\n",
            "[1820 | 8260.96] loss=0.74 avg=1.01\n",
            "[1825 | 8283.61] loss=0.68 avg=1.01\n",
            "[1830 | 8306.31] loss=0.79 avg=1.01\n",
            "[1835 | 8328.98] loss=0.81 avg=1.00\n",
            "[1840 | 8351.64] loss=0.79 avg=1.00\n",
            "[1845 | 8374.31] loss=0.80 avg=1.00\n",
            "[1850 | 8396.96] loss=0.85 avg=1.00\n",
            "[1855 | 8419.64] loss=0.76 avg=1.00\n",
            "[1860 | 8442.31] loss=0.78 avg=0.99\n",
            "[1865 | 8464.97] loss=0.73 avg=0.99\n",
            "[1870 | 8487.63] loss=0.73 avg=0.99\n",
            "[1875 | 8510.27] loss=0.79 avg=0.99\n",
            "[1880 | 8532.91] loss=0.71 avg=0.98\n",
            "[1885 | 8555.54] loss=0.82 avg=0.98\n",
            "[1890 | 8578.15] loss=0.70 avg=0.98\n",
            "[1895 | 8600.78] loss=0.74 avg=0.98\n",
            "[1900 | 8623.42] loss=0.80 avg=0.98\n",
            "[1905 | 8646.08] loss=0.71 avg=0.97\n",
            "[1910 | 8668.74] loss=0.87 avg=0.97\n",
            "[1915 | 8691.40] loss=0.77 avg=0.97\n",
            "[1920 | 8714.06] loss=0.83 avg=0.97\n",
            "[1925 | 8736.74] loss=0.85 avg=0.97\n",
            "[1930 | 8759.40] loss=0.71 avg=0.96\n",
            "[1935 | 8782.06] loss=0.80 avg=0.96\n",
            "[1940 | 8804.72] loss=0.77 avg=0.96\n",
            "[1945 | 8827.40] loss=0.66 avg=0.96\n",
            "[1950 | 8850.07] loss=0.77 avg=0.96\n",
            "[1955 | 8872.74] loss=0.76 avg=0.95\n",
            "[1960 | 8895.39] loss=0.80 avg=0.95\n",
            "[1965 | 8918.07] loss=0.82 avg=0.95\n",
            "[1970 | 8940.73] loss=0.71 avg=0.95\n",
            "[1975 | 8963.39] loss=0.72 avg=0.95\n",
            "[1980 | 8986.07] loss=0.79 avg=0.94\n",
            "[1985 | 9008.73] loss=0.84 avg=0.94\n",
            "[1990 | 9031.39] loss=0.73 avg=0.94\n",
            "[1995 | 9054.07] loss=0.76 avg=0.94\n",
            "[2000 | 9076.72] loss=0.89 avg=0.94\n",
            "Saving checkpoint/run1/model-2000\n",
            "======== SAMPLE 1 ========\n",
            " in the East region<|endoftext|>\n",
            "<|startoftext|>I hope I did not say too much last night Thanks for\n",
            "\n",
            "[2005 | 9102.03] loss=0.77 avg=0.94\n",
            "[2010 | 9124.76] loss=0.74 avg=0.93\n",
            "[2015 | 9147.46] loss=0.72 avg=0.93\n",
            "[2020 | 9170.08] loss=0.73 avg=0.93\n",
            "[2025 | 9192.73] loss=0.81 avg=0.93\n",
            "[2030 | 9215.40] loss=0.80 avg=0.93\n",
            "[2035 | 9238.09] loss=0.84 avg=0.93\n",
            "[2040 | 9260.81] loss=0.68 avg=0.92\n",
            "[2045 | 9283.50] loss=0.66 avg=0.92\n",
            "[2050 | 9306.19] loss=0.69 avg=0.92\n",
            "[2055 | 9328.88] loss=0.72 avg=0.92\n",
            "[2060 | 9351.52] loss=0.70 avg=0.92\n",
            "[2065 | 9374.16] loss=0.75 avg=0.91\n",
            "[2070 | 9396.80] loss=0.76 avg=0.91\n",
            "[2075 | 9419.43] loss=0.77 avg=0.91\n",
            "[2080 | 9442.08] loss=0.73 avg=0.91\n",
            "[2085 | 9464.74] loss=0.68 avg=0.91\n",
            "[2090 | 9487.39] loss=0.82 avg=0.91\n",
            "[2095 | 9510.04] loss=0.75 avg=0.90\n",
            "[2100 | 9532.70] loss=0.68 avg=0.90\n",
            "[2105 | 9555.35] loss=0.69 avg=0.90\n",
            "[2110 | 9578.03] loss=0.78 avg=0.90\n",
            "[2115 | 9600.70] loss=0.63 avg=0.90\n",
            "[2120 | 9623.33] loss=0.67 avg=0.89\n",
            "[2125 | 9645.99] loss=0.69 avg=0.89\n",
            "[2130 | 9668.65] loss=0.70 avg=0.89\n",
            "[2135 | 9691.33] loss=0.66 avg=0.89\n",
            "[2140 | 9713.97] loss=0.68 avg=0.89\n",
            "[2145 | 9736.66] loss=0.82 avg=0.88\n",
            "[2150 | 9759.30] loss=0.66 avg=0.88\n",
            "[2155 | 9781.96] loss=0.61 avg=0.88\n",
            "[2160 | 9804.61] loss=0.71 avg=0.88\n",
            "[2165 | 9827.23] loss=0.63 avg=0.88\n",
            "[2170 | 9849.86] loss=0.71 avg=0.87\n",
            "[2175 | 9872.49] loss=0.72 avg=0.87\n",
            "[2180 | 9895.13] loss=0.72 avg=0.87\n",
            "[2185 | 9917.79] loss=0.66 avg=0.87\n",
            "[2190 | 9940.47] loss=0.69 avg=0.87\n",
            "[2195 | 9963.14] loss=0.79 avg=0.87\n",
            "[2200 | 9985.82] loss=0.78 avg=0.86\n",
            "======== SAMPLE 1 ========\n",
            ">Gerald Attached is the draft document Please review and give me your comments by Monday morning August Thanks<|endoftext|>\n",
            "<|\n",
            "\n",
            "[2205 | 10008.84] loss=0.70 avg=0.86\n",
            "[2210 | 10031.52] loss=0.71 avg=0.86\n",
            "[2215 | 10054.22] loss=0.71 avg=0.86\n",
            "[2220 | 10076.89] loss=0.76 avg=0.86\n",
            "[2225 | 10099.58] loss=0.70 avg=0.86\n",
            "[2230 | 10122.29] loss=0.71 avg=0.86\n",
            "[2235 | 10144.99] loss=0.69 avg=0.85\n",
            "[2240 | 10167.67] loss=0.60 avg=0.85\n",
            "[2245 | 10190.31] loss=0.61 avg=0.85\n",
            "[2250 | 10212.95] loss=0.71 avg=0.85\n",
            "[2255 | 10235.58] loss=0.65 avg=0.85\n",
            "[2260 | 10258.22] loss=0.68 avg=0.84\n",
            "[2265 | 10280.87] loss=0.67 avg=0.84\n",
            "[2270 | 10303.54] loss=0.75 avg=0.84\n",
            "[2275 | 10326.19] loss=0.71 avg=0.84\n",
            "[2280 | 10348.85] loss=0.66 avg=0.84\n",
            "[2285 | 10371.51] loss=0.64 avg=0.84\n",
            "[2290 | 10394.17] loss=0.74 avg=0.84\n",
            "[2295 | 10416.86] loss=0.60 avg=0.83\n",
            "[2300 | 10439.55] loss=0.72 avg=0.83\n",
            "[2305 | 10462.23] loss=0.64 avg=0.83\n",
            "[2310 | 10484.92] loss=0.71 avg=0.83\n",
            "[2315 | 10507.59] loss=0.64 avg=0.83\n",
            "[2320 | 10530.27] loss=0.70 avg=0.83\n",
            "[2325 | 10552.95] loss=0.68 avg=0.82\n",
            "[2330 | 10575.63] loss=0.67 avg=0.82\n",
            "[2335 | 10598.31] loss=0.60 avg=0.82\n",
            "[2340 | 10620.96] loss=0.61 avg=0.82\n",
            "[2345 | 10643.60] loss=0.62 avg=0.82\n",
            "[2350 | 10666.24] loss=0.69 avg=0.81\n",
            "[2355 | 10688.87] loss=0.67 avg=0.81\n",
            "[2360 | 10711.50] loss=0.71 avg=0.81\n",
            "[2365 | 10734.14] loss=0.65 avg=0.81\n",
            "[2370 | 10756.79] loss=0.60 avg=0.81\n",
            "[2375 | 10779.45] loss=0.63 avg=0.81\n",
            "[2380 | 10802.12] loss=0.58 avg=0.80\n",
            "[2385 | 10824.77] loss=0.62 avg=0.80\n",
            "[2390 | 10847.43] loss=0.61 avg=0.80\n",
            "[2395 | 10870.10] loss=0.64 avg=0.80\n",
            "[2400 | 10892.76] loss=0.77 avg=0.80\n",
            "======== SAMPLE 1 ========\n",
            "oftext|>\n",
            "<|startoftext|>Yes I will follow up with you on that<|endoftext|>\n",
            "<|\n",
            "\n",
            "[2405 | 10915.74] loss=0.70 avg=0.80\n",
            "[2410 | 10938.40] loss=0.66 avg=0.80\n",
            "[2415 | 10961.06] loss=0.65 avg=0.79\n",
            "[2420 | 10983.70] loss=0.68 avg=0.79\n",
            "[2425 | 11006.34] loss=0.65 avg=0.79\n",
            "[2430 | 11029.00] loss=0.55 avg=0.79\n",
            "[2435 | 11051.66] loss=0.62 avg=0.79\n",
            "[2440 | 11074.34] loss=0.58 avg=0.79\n",
            "[2445 | 11096.97] loss=0.59 avg=0.78\n",
            "[2450 | 11119.53] loss=0.67 avg=0.78\n",
            "[2455 | 11142.10] loss=0.60 avg=0.78\n",
            "[2460 | 11164.78] loss=0.58 avg=0.78\n",
            "[2465 | 11187.45] loss=0.59 avg=0.78\n",
            "[2470 | 11210.07] loss=0.66 avg=0.78\n",
            "[2475 | 11232.72] loss=0.56 avg=0.77\n",
            "[2480 | 11255.37] loss=0.71 avg=0.77\n",
            "[2485 | 11278.05] loss=0.55 avg=0.77\n",
            "[2490 | 11300.72] loss=0.59 avg=0.77\n",
            "[2495 | 11323.37] loss=0.58 avg=0.77\n",
            "[2500 | 11346.00] loss=0.63 avg=0.77\n",
            "Saving checkpoint/run1/model-2500\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "file_name = 'train_data_gpt.csv'\n",
        "\n",
        "# # for faster dataloading in model uncomment below\n",
        "# file_name = 'text_encoded.npz'\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              batch_size=2, # total of 2048 samples per batch\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=2500,  # training-steps \n",
        "              restore_from='latest', # finetune from latest finetuned GPT-2\n",
        "              overwrite=True,\n",
        "              run_name='run1',\n",
        "              sample_length=30,\n",
        "              print_every=5,  # how much delayed iters-loss to print\n",
        "              sample_every=200,  # Number of training-steps AFTER WHICH to print output examples\n",
        "              save_every=500    # training-steps after which to save model\n",
        "              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8GBwr22d8_x"
      },
      "outputs": [],
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-UtDO317_Ol"
      },
      "source": [
        "### GPT-2 Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qX9DCgOP7-u9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20265c00-6db3-48ed-a24b-5d0f057a1a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "gpt2.mount_gdrive()\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MyWESsC8Lim",
        "outputId": "2f6257cb-e65d-48ec-d765-c0f7095cb921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-2500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-2500\n"
          ]
        }
      ],
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.backend.clear_session()"
      ],
      "metadata": {
        "id": "ZhqhfXprvg7W"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CTAWkBXvd86f"
      },
      "outputs": [],
      "source": [
        "def predict_sent(s,l=30):\n",
        "    prefix=\"<|startoftext|> \"+s\n",
        "    p = gpt2.generate(sess,\n",
        "                prefix=prefix,\n",
        "                truncate=\"<|endoftext|>\",\n",
        "                length=l,\n",
        "                run_name='run1',\n",
        "                temperature=0.7,\n",
        "                include_prefix=True,    # this isn't working (and is an open issue on the library's github), hence we truncate manually\n",
        "                return_as_list=True\n",
        "                )[0]\n",
        "                \n",
        "    p = p[len(prefix):]\n",
        "    return p.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyYQ-HaIsZMJ",
        "outputId": "658535f4-5100-474d-f84c-38f883d9f26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Final BLEU score over 100 random train-samples is: 0.09694239354106254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-2500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 50/100 [08:42<14:35, 17.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-2500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [17:49<00:00, 10.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Final BLEU score over 100 random test-samples is: 0.17828866606183447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# BLEU Score on data; we take just 100 samples as running GPT-2 is expensive \n",
        "\n",
        "# TRAIN DATA\n",
        "MAX_LENGTH = 30\n",
        "import nltk.translate.bleu_score as bleu\n",
        "sample = train_sequences.sample(100,ignore_index=True,replace=False,random_state=0)\n",
        "reference_inp = sample.enc_seq.str.lower().values.tolist()\n",
        "reference_tar = sample.dec_seq.str.lower().values.tolist()\n",
        "reference_tar = [[ele.strip().split()] for ele in reference_tar]  # changing to the format 'corpus_bleu' takes in\n",
        "prediction = []\n",
        "ct = 0\n",
        "for sent in tqdm(reference_inp):\n",
        "    # To avoid ram-usage-overflow issue \n",
        "    if ct%50==0:\n",
        "        tf.keras.backend.clear_session()\n",
        "        sess = gpt2.start_tf_sess()\n",
        "        gpt2.load_gpt2(sess, run_name='run1')\n",
        "\n",
        "    p = predict_sent(sent)\n",
        "    prediction.append(p.lower().split())\n",
        "    ct += 1\n",
        "\n",
        "# sf = bleu.SmoothingFunction()  ,smoothing_function=sf.method1\n",
        "print('\\nThe Final BLEU score over 100 random train-samples is: {}'.format(bleu.corpus_bleu(reference_tar, prediction)))\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# TEST DATA\n",
        "MAX_LENGTH = 30\n",
        "import nltk.translate.bleu_score as bleu\n",
        "sample = test_sequences.sample(100,ignore_index=True,replace=False,random_state=0)\n",
        "reference_inp = sample.enc_seq.str.lower().values.tolist()\n",
        "reference_tar = sample.dec_seq.str.lower().values.tolist()\n",
        "reference_tar = [[ele.strip().split()] for ele in reference_tar]  # changing to the format 'corpus_bleu' takes in\n",
        "prediction = []\n",
        "ct = 0\n",
        "for sent in tqdm(reference_inp):\n",
        "    # To avoid ram-usage-overflow issue \n",
        "    if ct%50==0:\n",
        "        tf.keras.backend.clear_session()\n",
        "        sess = gpt2.start_tf_sess()\n",
        "        gpt2.load_gpt2(sess, run_name='run1')\n",
        "\n",
        "    p = predict_sent(sent)\n",
        "    prediction.append(p.lower().split())\n",
        "    ct += 1\n",
        "\n",
        "# sf = bleu.SmoothingFunction()  ,smoothing_function=sf.method1\n",
        "print('\\nThe Final BLEU score over 100 random test-samples is: {}'.format(bleu.corpus_bleu(reference_tar, prediction)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bleu-Score for GPT-2 (small) on a sample of *100* test-sentences after *2500* training-steps is **0.18** with categorical loss of **0.77**"
      ],
      "metadata": {
        "id": "spmDK-3z7E98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict for sample sentences\n",
        "def predict_samples(data,k=10):\n",
        "    for _ in range(k):\n",
        "        idx = np.random.choice(data.shape[0])\n",
        "        input_sent = data.iloc[idx].enc_seq\n",
        "        target_sent = data.iloc[idx].dec_seq\n",
        "        print(\"Input-Sentence:\\n\",input_sent)\n",
        "        print('='*130)\n",
        "        print(\"Target-Sentence:\\n\",target_sent)\n",
        "        print('='*130)\n",
        "        p = predict_sent(input_sent)\n",
        "        print(\"Predicted-Sentence:\\n\",p)\n",
        "        print('x'*130)\n",
        "\n",
        "predict_samples(train_sequences)\n",
        "print()\n",
        "print('-*'*130)\n",
        "predict_samples(test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CWkCI_vv4ki",
        "outputId": "dd4716aa-316b-4d82-85d9-08ce00154d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input-Sentence:\n",
            " Should not you and Rick\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " do that together so as not to have confusion\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " do it together as well\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " I have confirmed the Vendor Number in Global Counterparty is we are still waiting for the\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " SAP Customer Job to complete\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " SAP Customer Job to complete\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " HourAhead No ancillary schedules awarded No\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " variances detected LOG PARSING FILE PortlandWestDeskCalifornia SchedulingISO Final Schedules txt\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " variances detected LOG PARSING FILE PortlandWestDeskCalifornia SchedulingISO Final Schedules txt\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " H Fletcher I do not see anything scheduled on this deal\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " number Can you tell me if we have anything scheduled on this deal number for this contract please\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " number Can you tell me if we have anything scheduled on this counterparty\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " This seems principally to be a help now sort of exercise but\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " perhaps there is a business opportunity in it for EES Thoughts\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " perhaps there is a useful correlation there ENA\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Seems like we need to reference Measurement activities as well What about the work related to the ROW Mary\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " Kay\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " Kay\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " What would you do to my on Just\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " read it great article Have fun in Costa Rica I hear it is amazing\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " read it great article Have fun in Costa Rica I hear it is amazing Thought you might find it fun Matt\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " I will be back Wednesday night Please leave me a message if you need\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " anything Hope your presentation goes well\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " anything Thanks\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " We have received the following executed\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " of ISDA Agreement Effective March for all products with the exception Weather pulp and paper and foreign exchange\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " of ISDA Agreement Effective March through March for all products with no transaction below collateral\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " No problem by the way I heard you guess\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " were able to get the VNG deal completed that is great news Congratulations BT\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " were included in the simulation session go check it out\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Input-Sentence:\n",
            " what do you want for your\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " round pick on ok TSE Compaq Computer Corporation\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " round pick I got your round for lunch pick\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " your last sentence sounded like an indian speaking english i was wondering why\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " you did not respond jury duty sucks huh have you been going out at all\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " you did not address the charge diana\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " will someone from the group be at the reception here in Houston to get the scoop Maybe check with to see if he is going\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " also Thanks\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " to be there\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Great How about noon in the lobby I would love to look at samples pics but maybe over the\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " weekend\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " weekend\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " call me again when you get a chance i lost\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " your number\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " your number again and someone at your place gave me your number again and forgot your name\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " are those lsu recruits where can\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " i see that stuff\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " you put the players\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " here will be a floor meeting today in room with and plan to attend rading Assistant to off\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " fax cell\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " fax cell\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " i have a dinner to go\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " to\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " to tonight call back at\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " What does this mean in terms of\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " money in out\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " remediation\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " This request has been pending approval for days\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " and you are the alternate Please click to review and act upon this request ed For enehouhoustoncommonResearch Read Write Type Directory\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " and you are the alternate Please click to review and act upon this request ed For enehouhoustoncommonResearch Read Type Directory<|endof\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latency-check-GPT"
      ],
      "metadata": {
        "id": "HE4Y4VHPriop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def get_latency():\n",
        "    t = []\n",
        "    sample = test_sequences.sample(60,ignore_index=True,replace=False,random_state=0) # as more than 60 crashes the ram\n",
        "    reference_inp = sample.enc_seq.str.lower().values.tolist()\n",
        "    ct = 0\n",
        "    for sent in tqdm(reference_inp):\n",
        "        a = time.time()\n",
        "        p = predict_sent(sent)\n",
        "        b = time.time()\n",
        "        ms = (b-a) * 1000 # time in milliseconds\n",
        "        t.append(ms)\n",
        "        ct += 1\n",
        "\n",
        "    return t"
      ],
      "metadata": {
        "id": "7bCZUgt2rkvA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = get_latency()\n",
        "np.mean(t),np.percentile(t,90),np.percentile(t,99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVknx1-NrlVA",
        "outputId": "5e819982-9d42-4b85-e285-ba9b63b213de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [11:53<00:00, 11.88s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11882.10867245992, 19693.22018623352, 21115.62076807022)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5 text-to-text transformer Modelling (Base-220 Million params)"
      ],
      "metadata": {
        "id": "dqF7M0gQrG3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training T5 transformer is very-hard and expensive, we use Kaggle kernels to finetune it hence no logs are available"
      ],
      "metadata": {
        "id": "ifY7XNTYAsFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slight changes for data\n",
        "train_sequences = train_sequences.rename(columns={\"dec_seq\":\"target_text\", \"enc_seq\":\"source_text\"})\n",
        "test_sequences = test_sequences.rename(columns={\"dec_seq\":\"target_text\", \"enc_seq\":\"source_text\"})\n",
        "\n",
        "print(train_sequences.shape,test_sequences.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84dLc0X95uHn",
        "outputId": "5f1cc26a-b8db-404d-b90f-aa8773093db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(735089, 2) (183306, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SimpleT5()\n",
        "# model.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n",
        "\n",
        "# model.train(train_df=X_train,\n",
        "#             eval_df=X_test, \n",
        "#             source_max_token_len=50, \n",
        "#             target_max_token_len=50, \n",
        "#             max_epochs=2, use_gpu=True,\n",
        "#             batch_size=64)"
      ],
      "metadata": {
        "id": "D2Lh9iDurPYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained on Kaggle hence no logs are available\n",
        "* The model is finetuned for 2 full epochs, after which it started *severly overfitting*.\n",
        "* Final **train-loss = 1.95** & **val-loss = 2.47**."
      ],
      "metadata": {
        "id": "99xWago6ZKbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T5 Inference"
      ],
      "metadata": {
        "id": "iLyoeQOyUbMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the finetuned model \n",
        "import gdown\n",
        "link = \"https://drive.google.com/drive/folders/1NDyWGYICKcLNJMIZ0KJ5rYxLbcb_jJZD?usp=sharing\"\n",
        "gdown.download_folder(link,quiet=True)\n",
        "\n",
        "model = SimpleT5()\n",
        "model.load_model(\"t5\",\"./outputs/simplet5-epoch-1-train-loss-1.954-val-loss-2.4739\")"
      ],
      "metadata": {
        "id": "k9mJAEs2ZN32"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sent_t5(s,l=50):\n",
        "    p = model.predict(s,max_length=l,num_beams=4,length_penalty=1.5)   # length penalty is set as T5 predictions are short      \n",
        "    p = p[0].split()    \n",
        "    return p"
      ],
      "metadata": {
        "id": "fmyYUe_vhtG6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate bleu score\n",
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "\n",
        "#                                   TRAIN DATA BLEU SCORE\n",
        "sample = train_sequences.sample(100,ignore_index=True,replace=False,random_state=0)\n",
        "reference_inp = sample.source_text.str.lower().values.tolist()\n",
        "\n",
        "reference_tar = sample.target_text.str.lower().values.tolist()\n",
        "reference_tar = [[ele.strip().split()] for ele in reference_tar] \n",
        "prediction = []\n",
        "\n",
        "for sent in tqdm(reference_inp):\n",
        "    p = predict_sent_t5(sent)\n",
        "    prediction.append(p)\n",
        "\n",
        "print('\\nThe Final BLEU score over 100 random Trining-samples is: {}'.format(bleu.corpus_bleu(reference_tar, prediction)))\n",
        "\n",
        "\n",
        "\n",
        "#                                   TEST DATA BLEU SCORE\n",
        "\n",
        "\n",
        "sample = test_sequences.sample(100,ignore_index=True,replace=False,random_state=0)\n",
        "reference_inp = sample.source_text.str.lower().values.tolist()\n",
        "\n",
        "reference_tar = sample.target_text.str.lower().values.tolist()\n",
        "reference_tar = [[ele.strip().split()] for ele in reference_tar] \n",
        "prediction = []\n",
        "\n",
        "for sent in tqdm(reference_inp):\n",
        "    p = predict_sent_t5(sent)\n",
        "    prediction.append(p)\n",
        "\n",
        "print('\\nThe Final BLEU score over 100 random Test-samples is: {}'.format(bleu.corpus_bleu(reference_tar, prediction)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7BUuwz0w-BX",
        "outputId": "dbcb955f-9bf5-4458-b368-ab906dbea66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:23<00:00,  1.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Final BLEU score over 100 random Trining-samples is: 0.11110997078057672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:24<00:00,  1.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Final BLEU score over 100 random Test-samples is: 0.07522400762797224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_samples(data,k=10):\n",
        "    '''\n",
        "    Predict a few sample outputs for T5 Model\n",
        "    '''\n",
        "    for _ in range(k):\n",
        "        idx = np.random.choice(data.shape[0])\n",
        "        input_sent = data.iloc[idx].source_text\n",
        "        target_sent = data.iloc[idx].target_text\n",
        "        print(\"Input-Sentence:\\n\",input_sent)\n",
        "        print('='*130)\n",
        "        print(\"Target-Sentence:\\n\",target_sent)\n",
        "        print('='*130)\n",
        "        p = ' '.join(predict_sent_t5(input_sent))\n",
        "        print(\"Predicted-Sentence:\\n\",p)\n",
        "        print('x'*130)\n",
        "\n",
        "predict_samples(train_sequences)\n",
        "print()\n",
        "print('-*'*130)\n",
        "predict_samples(test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO9dtzcejPvm",
        "outputId": "1a0ff7a4-9830-4af5-8deb-44dedaf09393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input-Sentence:\n",
            " Susan Can you please set\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " up the attached book for the estate\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " up a call with to discuss\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " The following expense report is ready for L Kimberly Status last changed\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " February CR Amount Due To approve this expense report click on the following link for Concur\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " To approve this expense report click on the following link for Concur\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " just wanted to let you know i got back from cayman yesterday and it was low hi\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " it is going to rock\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " how was your workout\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Invoice was a March invoice due It was\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " not generated when the invoice request was made\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " billed as of March Please let me know if you need anything else\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " We are planning on bringing if\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " that is OK\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " you are available\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Attached is the final Thanks\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " to all for help and comments Best Jeff\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " for your help\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " How did we get comfortable with MG charging commission on transactions In fact its the\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " whole thing about them using agreements which are offline\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " same thing we do with other companies\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " The following are the latest releases regarding\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " the status of the credit facility that will provide that LOC of M for the Red Rock expansion Regards\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " the PG E ISDA Master Agreement\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " In regard to is note below please confirm the\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " entities responsible for the cost of Kyle is trip Thank you Kim\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " above referenced counterparty and let me know if you have any questions Thank you Kim\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " This looks good to me We will\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " track down Janet if that is ok with you Kay\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " discuss at the staff meeting tomorrow\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "\n",
            "-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*\n",
            "Input-Sentence:\n",
            " Well it looks close between our two fantasy football teams I will pull for your players tonight if you cheer\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " for ours\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " on our football team\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Attached is the final version of the Master Purchase Sale Agreement for let me know if you need\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " anything else\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " anything else\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Say it ai not so R Brant your request here are my phone office home\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " cell\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " cell fax\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " The dates work on his schedule I can print this email and show it to him or you could put\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " together another email It is your choice Thanks\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " it in his calendar\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " If you are on the computer could you log into AOL I do not have a phone near me\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " to call you on we need to go over these curves\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " and can not find my password\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " has been created and entered in Sitara L Taylor create and enter into sitara a ticket based on the following Counterparty Meter Volume Price\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " d IF HSC less J Farmer\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " dth d\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Angie This is to confirm breakfast for\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " on Thursday November m in the Spring House Cafe Hyatt Regency Hill Country Resort Many Thanks Liz\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " the following a m Please let me know if you have any questions\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " sugar what is going on I love and miss you Call me\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " if you get a chance Love you Love you The Pigs\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " when you get a chance Love me\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " I think that this is what was referring\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " to If not please let me kn ow Best Jeff\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " to I have not seen anything on this site yet\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
            "Input-Sentence:\n",
            " Hi Stan Not to rush you but do you have an\n",
            "==================================================================================================================================\n",
            "Target-Sentence:\n",
            " estimate of when you will have completed your review memo\n",
            "==================================================================================================================================\n",
            "Predicted-Sentence:\n",
            " answer to the question below I would like to discuss this with you shortly\n",
            "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latency-check-T5"
      ],
      "metadata": {
        "id": "vh-QEhSxrn6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def get_latency():\n",
        "    t = []\n",
        "    sample = test_sequences.sample(100,ignore_index=True,replace=False,random_state=0) # as more than 60 crashes the ram\n",
        "    reference_inp = sample.enc_seq.str.lower().values.tolist()\n",
        "    ct = 0\n",
        "    for sent in tqdm(reference_inp):\n",
        "        a = time.time()\n",
        "        p = predict_sent_t5(sent)\n",
        "        b = time.time()\n",
        "        ms = (b-a) * 1000 # time in milliseconds\n",
        "        t.append(ms)\n",
        "        ct += 1\n",
        "\n",
        "    return t"
      ],
      "metadata": {
        "id": "qeRYnSaIrn6m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = get_latency()\n",
        "np.mean(t),np.percentile(t,90),np.percentile(t,99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAYeEkj4rn6m",
        "outputId": "52064068-a48c-4d3f-80ed-7ffce91cd3cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:28<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(882.7659273147583, 1364.3655061721804, 2461.0164952278215)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "rKttvn-iRrNy"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kcnBGYLLgKvj",
        "XO2zYl8rf2MN",
        "A__ypVCM_V_Q",
        "E-UtDO317_Ol",
        "HE4Y4VHPriop",
        "dqF7M0gQrG3v",
        "iLyoeQOyUbMR",
        "vh-QEhSxrn6m"
      ],
      "name": "SOTA_Modelling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}