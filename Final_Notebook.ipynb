{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final-Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "import pickle\n",
        "import email\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "from dateutil import parser\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "import re\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "import nltk.translate.bleu_score as bleu\n"
      ],
      "metadata": {
        "id": "VNGv-ROFFdK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "JICvTke7DnAQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XwWBYTGk8-JF"
      },
      "outputs": [],
      "source": [
        "def remove_extensions(text):\n",
        "    '''\n",
        "    We removed attachments while extracting body but not the name of these attachments\n",
        "    removing attachment_names based on what i encountered in subject and body\n",
        "    '''\n",
        "    ext_patterns = [\"\\S+\\.doc\",\"\\S+\\.jpeg\",\"\\S+\\.jpg\",\"\\S+\\.gif\",\"\\S+\\.csv\",\"\\S+\\.ppt\",\"\\S+\\.dat\",\"\\S+\\.xml\",\"\\S+\\.xls\",\"\\S+\\.sql\",\"\\S+\\.nsf\",\"\\S+\\.jar\",\"\\S+\\.bin\",\"\\S+\\.txt\"]\n",
        "    pattern = '|'.join(ext_patterns)\n",
        "    text = re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "def remove_personal_name(text):\n",
        "    '''\n",
        "    Helper function to Filter out names using NER\n",
        "    '''\n",
        "    s = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
        "    for ele in s:\n",
        "        if isinstance(ele, nltk.Tree):\n",
        "            if ele.label()=='PERSON':\n",
        "                for word,pos_tag in ele:\n",
        "                    try:     # words containing a special character will raise an error so handling it, these words weren't a name so we can safely skip it\n",
        "                        val = re.sub(word,'',text)\n",
        "                        text = val\n",
        "                    except:\n",
        "                        continue\n",
        "    return text\n",
        "\n",
        "def decontracted(phrase):\n",
        "    \"\"\"\n",
        "    Returns decontracted phrases\n",
        "    \"\"\"\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"ain\\'t\", \"am not\", phrase)\n",
        "    phrase = re.sub(r\"let\\'s\", \"let us\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def remove_timestamps(text):\n",
        "    '''\n",
        "    Remove all types of 'text' data from timestamps\n",
        "    '''\n",
        "    text = text.replace('AM','')\n",
        "    text = text.replace('PM','')\n",
        "    text = text.replace('A.M.','')\n",
        "    text = text.replace('P.M.','')\n",
        "    text = text.replace('a.m.','')\n",
        "    text = text.replace('p.m.','')\n",
        "    text = re.sub(r\"\\bam\\b\",'',text)\n",
        "    text = re.sub(r\"\\bpm\\b\",'',text)\n",
        "    return text\n",
        "\n",
        "def final_transform(text):\n",
        "    '''\n",
        "    We clean the full text/body using regex and other cleaning functions\n",
        "    '''\n",
        "    # remove URL's\n",
        "    remove_url = r'(www|http)\\S+'     # https://stackoverflow.com/a/40823105\n",
        "    remove_phone = '(\\+\\d{1,2}\\s)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}'   # ONLY US numbers for now --> https://stackoverflow.com/a/16699507\n",
        "\n",
        "    #remove ANY emails\n",
        "    remove_email = r'\\S+@\\S+'  # https://stackoverflow.com/a/64036475\n",
        "\n",
        "\n",
        "    pattern_list_1 = [remove_url,remove_phone,remove_email]\n",
        "\n",
        "    for pattern in pattern_list_1:\n",
        "        text = re.sub(pattern,'',text)\n",
        "\n",
        "    # remove attachment_names\n",
        "    text = remove_extensions(text)\n",
        "\n",
        "    # remove any word with digit\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    # remove any digit\n",
        "    text = re.sub('\\d','',text)\n",
        "\n",
        "    # remove text between <>,()\n",
        "    remove_tags = r'<.*>'\n",
        "    remove_brackets = r'\\(.*\\)'\n",
        "    remove_special_1 = r'\\\\|-'  # remove raw backslash or '-'\n",
        "    remove_colon = r'\\b[\\w]+:' # removes 'something:'\n",
        "\n",
        "    pattern_list_2 = [remove_tags,remove_brackets,remove_special_1,remove_colon]\n",
        "    for pattern in pattern_list_2:\n",
        "        text = re.sub(pattern,'',text)\n",
        "\n",
        "    # remove anything which is not a character,apostrophy ; remember to give a space on replacing with this\n",
        "    remove_nonchars = r'[^A-Za-z\\']'\n",
        "    text = re.sub(remove_nonchars,' ',text)\n",
        "\n",
        "    # remove AM/PM as we have a lot of timestamps in emails\n",
        "    text = remove_timestamps(text)\n",
        "\n",
        "    # remove personal names using named entity recognition\n",
        "    text = remove_personal_name(text)\n",
        "\n",
        "    # takes care of \\t & \\n ; remember to give a space on replacing with this\n",
        "    remove_space = r'\\s+'\n",
        "    text = re.sub(remove_space,' ',text)\n",
        "\n",
        "    # take care of apostrophies\n",
        "    text = decontracted(text)\n",
        "\n",
        "    # remove other junk\n",
        "    text = text.replace(\"IMAGE\",'')\n",
        "    text = re.sub(r\"\\bth\\b\",'',text)\n",
        "\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Functions"
      ],
      "metadata": {
        "id": "19qvvFCrHrkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model is to be initialized globally outside any function\n",
        "gpt2.mount_gdrive()\n",
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')\n",
        "\n",
        "def final_function_1(sent):\n",
        "    '''\n",
        "    * take inputs\n",
        "    * preprocess it\n",
        "    * make predictions from model & return\n",
        "    '''    \n",
        "\n",
        "    # check length of sentence\n",
        "    MAX_LEN = 30\n",
        "    sent = ' '.join(sent.strip().split()[:MAX_LEN])\n",
        "    # PREPROCESS\n",
        "    sent = final_transform(sent)\n",
        "    # inference\n",
        "    prefix=\"<|startoftext|> \"+sent\n",
        "    p = gpt2.generate(sess,\n",
        "                prefix=prefix,\n",
        "                truncate=\"<|endoftext|>\",\n",
        "                length=MAX_LEN,\n",
        "                run_name='run1',\n",
        "                temperature=0.7,\n",
        "                include_prefix=True,    \n",
        "                return_as_list=True\n",
        "                )[0]\n",
        "                \n",
        "    p = p[len(prefix):]\n",
        "    return p.strip()\n",
        "\n",
        "def final_function_2(inp,tar):\n",
        "    '''\n",
        "    * take inputs & targets\n",
        "    * predict output for input sentence\n",
        "    * calc bleu-score for (model-output & target) and return it\n",
        "    '''\n",
        "    out = final_function_1(inp)\n",
        "    hypothesis = out.split()\n",
        "    reference_tar = [tar.strip().split()]\n",
        "    return bleu.sentence_bleu(reference_tar,hypothesis)"
      ],
      "metadata": {
        "id": "i7IkP4Q79G15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d40f804-1437-4910-ec04-b0ba622173a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading checkpoint checkpoint/run1/model-2500\n",
            "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_function_1(\"Hey Rita, did you get\")"
      ],
      "metadata": {
        "id": "LY0sI_qw9GyN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "92eccc8d-de52-4421-a586-958d46e8d8d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a chance to look at the Credit Agreement and review the list of issues I put together in our meeting today'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_function_2(\"Hey Rita, did you get\",\"the previous email on the application\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxrTQ339vJm0",
        "outputId": "98f203af-c7fa-4e16-a28b-d7a4556ffe9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0732986305800918e-232"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "HsGCtcxEHwKO"
      }
    }
  ]
}